---
title: "Exercise 2"
output: html_document
date: "2024-02-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

####QUESTION1
library(academictwitteR) # for fetching Twitter data
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)

tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))

# head(tweets) 查看数据前几行

# colnames(tweets) 查看数据列

# 筛选出了特定的列，并重命名这些列，最后只包含重命名后的所选列
tweets <- tweets %>%
  select(user_username, text, created_at, user_name,
         retweet_count, like_count, quote_count) %>%
  rename(username = user_username,
         newspaper = user_name,
         tweet = text)
# 将原始推文文本转换为小写、拆分成单词，过滤掉非字母单词
tidy_tweets <- tweets %>% 
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

# 删除停止词
tidy_tweets <- tidy_tweets %>%
  filter(!word %in% stop_words$word)

# 查看情感词典
# get_sentiments("afinn")
# get_sentiments("bing")
# get_sentiments("nrc")

# 获取恐惧情绪的单词
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

# 与tidy_tweets数据框结合
tidy_tweets %>%
  inner_join(nrc_fear) %>%
  # 计算单词频次
  count(word, sort = TRUE)

# 创建了一个新的列date在tidy_tweets数据框中，该列是由created_at列转换而来的日期
tidy_tweets$date <- as.Date(tidy_tweets$created_at)

# 对数据框按日期排序
tidy_tweets <- tidy_tweets %>%
  arrange(date)

# 排序后每一行标出12345
tidy_tweets$order <- 1:nrow(tidy_tweets)

# 构建一个新的数据框，包含每个时间点的情感分布，并计算简化的情感得分
tweets_nrc_sentiment <- tidy_tweets %>%
  inner_join(get_sentiments("nrc")) %>% # 数据框和词典结合
  count(date, index = order %/% 1000, sentiment) %>% # 计算每个日期每个千行数据分组每种情感类型中的单词出现次数
  spread(sentiment, n, fill = 0) %>% # 将数据从长格式转换为宽格式
  mutate(sentiment = positive - negative) # 将函数用于计算一个情感得分

# 画图
tweets_nrc_sentiment %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25)

# 换一本词典看看
tidy_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("bing sentiment")

# 换一本词典看看
tidy_tweets %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("nrc sentiment")

# 换一本词典看看
tidy_tweets %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date, index = order %/% 1000) %>% 
  summarise(sentiment = sum(value)) %>% 
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("afinn sentiment")

# 换成特定领域的词典并画图
word <- c('death', 'illness', 'hospital', 'life', 'health',
          'fatality', 'morbidity', 'deadly', 'dead', 'victim')
value <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
mordict <- data.frame(word, value)
mordict

tidy_tweets %>%
  inner_join(mordict) %>%
  group_by(date, index = order %/% 1000) %>% 
  summarise(morwords = sum(value)) %>% 
  ggplot(aes(date, morwords)) +
  geom_bar(stat= "identity") +
  ylab("mortality words")

# 新向量
mordict <- c('death', 'illness', 'hospital', 'life', 'health',
             'fatality', 'morbidity', 'deadly', 'dead', 'victim')

# 添加一个辅助列、按日期分组以及汇总每组的单词计数
totals <- tidy_tweets %>%
  mutate(obs=1) %>%
  group_by(date) %>%
  summarise(sum_words = sum(obs))

# 制图
tidy_tweets %>%
  mutate(obs=1) %>%
  filter(grepl(paste0(mordict, collapse = "|"),word, ignore.case = T)) %>%
  group_by(date) %>%
  summarise(sum_mwords = sum(obs)) %>%
  full_join(totals, word, by="date") %>%
  mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
         pctmwords = sum_mwords/sum_words) %>%
  ggplot(aes(date, pctmwords)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  xlab("Date") + ylab("% mortality words")

# 转换为标准的日期格式
tweets$date <- as.Date(tweets$created_at)

# 文本日期关联
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date") 

# 删除标点
toks_news <- tokens(tweet_corpus, remove_punct = TRUE)

# 只选积极消极单词不选中性词进行评分
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]

toks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)

# 画图
dfmat_news_lsd <- dfm(toks_news_lsd) %>% 
  dfm_group(groups = date)

matplot(dfmat_news_lsd$date, dfmat_news_lsd, type = "l", lty = 1, col = 1:2,
        ylab = "Frequency", xlab = "")
grid()
legend("topleft", col = 1:2, legend = colnames(dfmat_news_lsd), lty = 1, bg = "white")


##QUESTION2
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r}
##Question2: Build your own (minimal) dictionary-based filter technique and plot the result
library(kableExtra)
library(tidyverse) 
library(readr) 
library(stringr) 
library(tidytext)
library(quanteda) 
library(textdata)
```

```{r}
library(academictwitteR)
```

```{r}
getwd()
```

```{r, eval = F}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
```

```{r, eval = F}
head(tweets)
colnames(tweets)
```

```{r, eval = F}
tweets <- tweets %>%
  select(user_username, text, created_at, user_name,
         retweet_count, like_count, quote_count) %>%
  rename(username = user_username,
         newspaper = user_name,
         tweet = text)
```


```{r, eval = F}
tidy_tweets <- tweets %>% 
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))
```

```{r, eval = F}
tidy_tweets <- tidy_tweets %>%
    filter(!word %in% stop_words$word)
```

```{r}
get_sentiments("bing")
```

```{r, eval = F}

bing_negative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

tidy_tweets %>%
  inner_join(bing_negative) %>%
  count(word, sort = TRUE)
```
```{r, eval = F}
tidy_tweets$date <- as.Date(tidy_tweets$created_at)

tidy_tweets <- tidy_tweets %>%
  arrange(date)

tidy_tweets$order <- 1:nrow(tidy_tweets)

```



```{r, eval = F}
tidy_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("bing sentiment")
```


#QUESTION 3
#start a corpus

as.numeric (text$newspaper) -> newspaper
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "newspaper")

#find a dictionary

data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]

#score

toks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)

# create a document document-feature matrix and group it by newspaper
dfmat_news_lsd <- dfm(toks_news_lsd) %>% 
  dfm_group(groups = newspaper)
  
  
#graph
matplot(dfmat_news_lsd$newspaper, dfmat_news_lsd, type = "l", lty = 1, col = 1:2,
        ylab = "Frequency", xlab = "")
grid()
legend("topleft", col = 1:2, legend = colnames(dfmat_news_lsd), lty = 1, bg = "white")